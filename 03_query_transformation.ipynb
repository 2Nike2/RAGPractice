{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.load import dumps, loads\n",
    "from pprint import pprint\n",
    "from operator import itemgetter\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain import hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web文書の取得、分割、リトリーバ作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    )\n",
    ")\n",
    "blog_docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
    "splits = text_splitter.split_documents(blog_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(splits, OpenAIEmbeddings())\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### マルチクエリプロンプト作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 異なる視点を得る為のマルチクエリプロンプト\n",
    "# あなたはAI言語モデルアシスタントです。あなたの仕事は、ベクトルデータベースから関連する文書を取得するために、\n",
    "# 与えられたユーザーの質問を5つの異なるバージョンで生成することです。\n",
    "# ユーザーの質問に対する複数の視点を生成することにより、距離ベースの類似性検索の一部の限界を克服する助けとなることがあなたの目標です。\n",
    "# 代替となる質問を改行で区切って提供してください。元の質問: {question}\n",
    "template = \"\"\"You are an AI language model assistant. Your task is to generate five \n",
    "different versions of the given user question to retrieve relevant documents from a vector \n",
    "database. By generating multiple perspectives on the user question, your goal is to help\n",
    "the user overcome some of the limitations of the distance-based similarity search. \n",
    "Provide these alternative questions separated by newlines. Original question: {question}\"\"\"\n",
    "prompt_perspectives = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### マルチクエリチェーンの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_queries = (\n",
    "    prompt_perspectives\n",
    "    | ChatOpenAI(temperature=0)\n",
    "    | StrOutputParser()\n",
    "    | (lambda x: x.split('\\n'))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### マルチクエリによる文書検索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_union(documents: list[list]):\n",
    "\n",
    "    # 各種要素数確認\n",
    "    print(\"親要素数:\", len(documents))\n",
    "    print(\"子要素数:\", [len(sublist) for sublist in documents])\n",
    "\n",
    "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
    "    unique_docs = list(set(flattened_docs))\n",
    "\n",
    "    return [loads(doc) for doc in unique_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is task decomposition for LLM agents?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_list_and_pass(x):\n",
    "    print('要素数:', len(x))\n",
    "    pprint(x)\n",
    "    return x\n",
    "\n",
    "retrieval_chain = generate_queries | print_list_and_pass | retriever.map() | get_unique_union # 作成したマルチクエリの確認を含めるチェーン\n",
    "# retrieval_chain = generate_queries | retriever.map() | get_unique_union # \n",
    "docs = retrieval_chain.invoke({'question': question})\n",
    "print('文書数:', len(docs))\n",
    "pprint(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 回答生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "final_rag_chain = (\n",
    "    {\"context\": retrieval_chain,\n",
    "     \"question\": itemgetter(\"question\")}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG-Fusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### マルチクエリプロンプト作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are a helpful assistant that generates multiple search queries based on a single input query. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (4 queries):\"\"\"\n",
    "prompt_rag_fusion = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_queries = (\n",
    "    prompt_rag_fusion\n",
    "    | ChatOpenAI(temperature=0)\n",
    "    | StrOutputParser()\n",
    "    | (lambda x: x.split('\\n'))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG-Fusionリランキング処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "\n",
    "    # 各種要素数確認\n",
    "    print(\"親要素数:\", len(results))\n",
    "    print(\"子要素数:\", [len(sublist) for sublist in results])\n",
    "\n",
    "    fused_scores = {}\n",
    "\n",
    "    for docs in results:\n",
    "        for rank, doc in enumerate(docs):\n",
    "            doc_str = dumps(doc)\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            previous_score = fused_scores[doc_str]\n",
    "            fused_scores[doc_str] += 1 / (rank + k)\n",
    "\n",
    "\n",
    "    reranked_results = [\n",
    "        (loads(doc), score) for doc, score in sorted(fused_scores.items(), key=lambda x:x[1], reverse=True)\n",
    "    ]\n",
    "    return reranked_results # 定数あるいは引数を指定してここで上位のもののみ取得できるように絞り込みを行う実装も可能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_chain_rag_fusion = generate_queries | print_list_and_pass | retriever.map()  | reciprocal_rank_fusion\n",
    "docs = retrieval_chain_rag_fusion.invoke({'question': question})\n",
    "print('文書数:', len(docs))\n",
    "pprint(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 回答生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    {\"context\": retrieval_chain_rag_fusion,\n",
    "     \"question\": itemgetter(\"question\")}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decompositionプロンプト作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 与えられた問題を部分問題に分解する為のDecompositionプロンプト\n",
    "# あなたは入力された質問に関連する複数のサブ質問を生成する役立つアシスタントです。\n",
    "# 目標は、入力を分離して回答可能な一連のサブ問題／サブ質問に分解することです。\n",
    "# 次の質問に関連する複数の検索クエリを生成してください: {question}\n",
    "# 出力（3つのクエリ）：\n",
    "template = \"\"\"You are a helpful assistant that generates multiple sub-questions related to an input question. \\n\n",
    "The goal is to break down the input into a set of sub-problems / sub-questions that can be answers in isolation. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (3 queries separated by new lines):\"\"\"\n",
    "prompt_decomposition = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decompositionチェーン作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_queries_decomposition = (\n",
    "    prompt_decomposition\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    "    | (lambda x: x.split('\\n'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decomposition実行\n",
    "question = \"What are the main components of an LLM-powered autonomous agent system?\"\n",
    "questions = generate_queries_decomposition.invoke({'question': question})\n",
    "pprint(questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decompositonで作成された質問の回答生成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decomposition作成質問を逐次追加するパターン"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ここに回答しないといけない質問があります：\n",
    "#\\n --- \\n {question} \\n --- \\n\n",
    "# ここに利用可能な背景の質問と回答のペアがあります：\n",
    "# \\n --- \\n {q_a_pairs} \\n --- \\n\n",
    "# 質問に関連する追加のコンテキストがここにあります：\n",
    "# \\n --- \\n {context} \\n --- \\n\n",
    "# 上記のコンテキストと背景の質問＋回答のペアを使用して、質問に答えてください：\\n {question}\n",
    "template = \"\"\"Here is the question you need to answer:\n",
    "\n",
    "\\n --- \\n {question} \\n --- \\n\n",
    "\n",
    "Here is any available background question + answer pairs:\n",
    "\n",
    "\\n --- \\n {q_a_pairs} \\n --- \\n\n",
    "\n",
    "Here is additional context relevant to the question: \n",
    "\n",
    "\\n --- \\n {context} \\n --- \\n\n",
    "\n",
    "Use the above context and any background question + answer pairs to answer the question: \\n {question}\n",
    "\"\"\"\n",
    "decomposition_prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 質問回答ペア文字列作成\n",
    "def format_qa_pair(question, answer):\n",
    "    formatted_string = \"\"\n",
    "    formatted_string += f\"Question: {question}\\nAnswer: {answer}\\n\\n\"\n",
    "    return formatted_string.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 回答生成\n",
    "q_a_pairs = ''\n",
    "for q in questions:\n",
    "    rag_chain = (\n",
    "        {\"context\": itemgetter(\"question\") | retriever,\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "        \"q_a_pairs\": itemgetter(\"q_a_pairs\")}\n",
    "    | decomposition_prompt\n",
    "    | llm\n",
    "    | StrOutputParser())\n",
    "\n",
    "    answer = rag_chain.invoke({\"question\": q, \"q_a_pairs\": q_a_pairs})\n",
    "    q_a_pair = format_qa_pair(q, answer)\n",
    "    q_a_pairs = q_a_pairs + \"\\n---\\n\"+ q_a_pair\n",
    "pprint(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decomposition作成質問を個別処理するパターン"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_rag = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# サブ質問のリストと対応する回答のリストを作成する処理\n",
    "def retrieve_and_rag(question, prompt_rag, sub_question_generator_chain):\n",
    "\n",
    "    sub_questions = sub_question_generator_chain.invoke({'question': question})\n",
    "    rag_results = []\n",
    "\n",
    "    for sub_question in sub_questions:\n",
    "\n",
    "        retrieved_docs = retriever.get_relevant_documents(sub_question)\n",
    "\n",
    "        answer = (\n",
    "            prompt_rag | llm | StrOutputParser()\n",
    "        ).invoke({'context': retrieved_docs, 'question': sub_question})\n",
    "        rag_results.append(answer)\n",
    "\n",
    "    return rag_results, sub_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers, questions = retrieve_and_rag(question, prompt_rag, generate_queries_decomposition)\n",
    "for temp_q, temp_a in zip(questions, answers):\n",
    "    print(f\"Question: {temp_q}\\nAnswer: {temp_a}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 複数質問回答ペア文字列作成処理\n",
    "def format_qa_pairs(questions, answers):\n",
    "    formatted_string = \"\"\n",
    "    for i, (question, answer) in enumerate(zip(questions, answers), start=1):\n",
    "        formatted_string += f\"Question {i}: {question}\\nAnswer {i}: {answer}\\n\\n\"\n",
    "    return formatted_string.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = format_qa_pairs(questions, answers)\n",
    "\n",
    "template = \"\"\"Here is a set of Q+A pairs:\n",
    "\n",
    "{context}\n",
    "\n",
    "Use these to synthesize an answer to the question: {question}\n",
    "\"\"\"\n",
    "\n",
    "final_rag_chain = (\n",
    "    prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "pprint(final_rag_chain.invoke({\"context\":context, \"question\":question}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step Back"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step Backプロンプトの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\n",
    "        \"input\": \"Could the members of The Police perform lawful arrests?\",\n",
    "        \"output\": \"what can the members of The Police do?\",\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Jan Sindel’s was born in what country?\",\n",
    "        \"output\": \"what is Jan Sindel’s personal history?\",\n",
    "    },\n",
    "]\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"system\", \"{output}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples\n",
    ")\n",
    "\n",
    "# あなたは世界の知識に精通しています。\n",
    "# あなたの仕事は、一歩引いて質問をより一般的な「一歩引いた質問」に言い換えることであり、それによって質問を回答しやすくすることです。\n",
    "# 以下にいくつかの例を示します：\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"\"\"You are an expert at world knowledge. Your task is to step back and paraphrase a question to a more generic step-back question, which is easier to answer. Here are a few examples:\"\"\",),\n",
    "        few_shot_prompt,\n",
    "        ('user', '{question}'),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_messages_and_pass_(x):\n",
    "    pprint(x.messages)\n",
    "    return x\n",
    "# generate_queries_step_back = prompt | print_messages_and_pass_ | ChatOpenAI(temperature=0) | StrOutputParser()\n",
    "generate_queries_step_back = prompt | ChatOpenAI(temperature=0) | StrOutputParser()\n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "print(generate_queries_step_back.invoke({\"question\": question}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 回答作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# あなたは世界知識の専門家です。これから質問をします。\n",
    "# 回答は包括的に、かつ与えられた文脈と関連性がある場合は矛盾しないようにしてください。関連性がなければ無視してください。\n",
    "# 通常(元質問)の文脈\n",
    "# 抽象化した質問の文脈\n",
    "# 元質問: {question}\n",
    "# 回答:\n",
    "response_prompt_template = \"\"\"You are an expert of world knowledge. I am going to ask you a question. Your response should be comprehensive and not contradicted with the following context if they are relevant. Otherwise, ignore them if they are not relevant.\n",
    "\n",
    "# {normal_context}\n",
    "# {step_back_context}\n",
    "\n",
    "# Original Question: {question}\n",
    "# Answer:\"\"\"\n",
    "response_prompt = ChatPromptTemplate.from_template(response_prompt_template)\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"normal_context\": RunnableLambda(lambda x: x[\"question\"]) | retriever,\n",
    "        \"step_back_context\": generate_queries_step_back | retriever,\n",
    "        \"question\": lambda x: x[\"question\"],\n",
    "    }\n",
    "    | response_prompt\n",
    "    | ChatOpenAI(temperature=0)\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(chain.invoke({\"question\": question}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HyDE(Hypothetical Document Embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 次の質問に答えるための科学論文の一説を書いてください\n",
    "template = \"\"\"Please write a scientific paper passage to answer the question\n",
    "Question: {question}\n",
    "Passage:\"\"\"\n",
    "prompt_hyde = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 仮説的な回答生成\n",
    "generate_docs_for_retrieval = (\n",
    "    prompt_hyde\n",
    "    | ChatOpenAI(temperature=0)\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "print(generate_docs_for_retrieval.invoke({\"question\":question}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 仮説的回答に似た文書の検索\n",
    "retrieval_chain = generate_docs_for_retrieval | retriever\n",
    "retrieved_docs = retrieval_chain.invoke({\"question\": question})\n",
    "pprint(retrieved_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 回答生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 複数文書結合処理\n",
    "def unify_docs(documents: list):\n",
    "    unified_str = '\\n---\\n'.join([document.page_content for document in documents])\n",
    "    return unified_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    {\"context\": RunnableLambda(lambda x: unify_docs(x[\"context\"])), \"question\": itemgetter(\"question\")}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(final_rag_chain.invoke({\"context\":retrieved_docs, \"question\":question}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
